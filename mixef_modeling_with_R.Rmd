---
title: "Mixed-Effect/Multilevel Modeling with R "
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## Load packages

We're going to use the following packages in this workshop. Let's load them.

```{r}
library(lme4)
library(ggplot2)
library(ggeffects)
```



## Simple linear regression review

Instead of using mathematical statistics, we'll try to motivate mixed-effect/multilevel models using simulation. Below we generate data from a straight line model. y is completely determined by x. We might say the intercept (3) and slope (2) are _fixed effects_.

```{r}
x <- 1:10
y <- 3 + 2*x
d <- data.frame(x, y)
plot(y ~ x, data = d)
```


Now let's add some noise to each observation. We'll use 10 random draws from a Normal distribution with mean 0 and standard deviation of 1.5. The `set.seed(1)` function ensures we all draw the same values. Now y looks associated with x, but not completely determined by x. We might say each observation has a _random effect_.

```{r}
set.seed(1)
noise <- rnorm(10, mean = 0, sd = 1.5)
d$y <- 3 + 2*x + noise
plot(y ~ x, data = d)
```

Now let's fit a simple linear model, or regression line, using the correct model we used to simulate the data. We use the `lm()` function for this. The formula "y ~ x" means we think the model is "y = intercept + slope*x".

```{r}
m <- lm(y ~ x, data = d)
summary(m)
```

Assuming our data came from a function of the form _y = a + b*x_ with noise sampled from a Normal distribution, the model estimates a as 2.7468 and b as 2.0821. Those are fairly close to the true values of 3 and 2. The noise is estimated to be from a Normal distribution with mean 0 and standard deviation 1.214 (Residual standard error). Again close to the true value of 1.5.

We might say the _fixed effects_ are about 2.75 and 2.08, and the _random effect_ is 1.24.

If we like we can fit this model to the data using the ggeffects package.

```{r}
plot(ggeffect(m, terms = "x"), add.data = TRUE)

```


## Motivation for mixed-effect/multilevel models

Pretend the previous example data was for one subject. What if we had 7 subjects, each with 10 observations? 

Let's simulate data where we add noise to the intercept that's specific to each subject. In other words each subject has a _random effect_. 


```{r}
# generate 7 id numbers, 10 each
id <- gl(n = 7, k = 10)

# repeat x times
x <- rep(1:10, 7)

# generate a random effect specific to each observation (n = 10 * 7)
set.seed(2)
obs_noise <- rnorm(7 * 10, mean = 0, sd = 1.5)

# generate a random effect specific to each subject (n = 7)
set.seed(3)
subj_noise <- rnorm(7, mean = 0, sd = 2)

# generate y; add subject random effect to intercept;
# rand_int[id] uses id as index numbers to repeat subj_noise values
y <- (3 + subj_noise[id]) + 2*x + obs_noise
d2 <- data.frame(id, y, x)
```

Now we have two fixed effects:
- Intercept = 3
- Slope = 2

And two random effects:
- Observation: N(0, 1.5)
- Subject: N(0, 2)

Let's visualize the data. We have lines with _different intercepts_ but similar slopes. That's because _we added each subject's random effect to the fixed intercept_. Any differences in slopes are due to random effects associated with each observation.

The argument `method = "lm"` below adds a regression line to each subject's values.

```{r}
ggplot(d2) +
  aes(x = x, y = y, color = id) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

How do we "work backwards" and recover the true intercept, slope and two random effect parameters?

**linear mixed-effect models, or multilevel models**

Because we are dealing with models with a mix of fixed and random effects, we sometimes call these **Mixed-Effect Models**. Another name is **Multilevel Models** because we're dealing with different levels of observations. (eg, we observe 7 subjects, but also observe 10 observations on each subject.)

Below we model y as a function of x plus an intercept (1) that is conditional on id (the subject) using the `lmer()` function from the lme4 package. Notice this is the _correct model_! The y value really is a function of x plus an intercept that varies between subjects.  

```{r}
me1 <- lmer(y ~ x + (1|id), data = d2)
summary(me1)
```

The _Fixed Effects_ section says that assuming this data came from a straight line model with an intercept and slope, the model estimates the intercept to be 2.65 and the slope to be 1.97. These are very close to the true values of 3 and 2. 

The _Random Effects_ section reports two standard deviations: one for id and and one called "Residual". The first is the estimate of the standard deviation of the normal distribution from which the subject-specific noise was drawn: 1.157. The second is the estimate of the standard deviation of the normal distribution from which the residual noise was drawn: 1.795. Recall the true values were 2 and 1.5, respectively. 

If we look at the coefficients of this model, notice everyone got their _own intercept_ but has the same slope. We have essentially fit a straight line model to each subject. This is sometimes called a _random intercept model_. Each subject has a random effect associated with the intercept.

```{r}
coef(me1)
```


This is one way to think of mixed-effect/multilevel modeling: _trying to work backward to determine the data generation process_. Our example was simple and we knew the data generating process because we simulated the data. In real life we never know if our model is correct (spoiler: it never is) and the data generating process is complicated.

## Checking assumptions

Notice two assumptions that `lmer()` made:

1. The variance (noise) is constant for both random effects.
2. The variance (noise) comes from a Normal distribution for both random effects

We can assess those assumptions with _diagnostic plots_.

Check constant variance for Residuals (aka, within subjects). Points should be evenly scattered around 0.

```{r}
plot(me1)
```

Check constant variance for Subjects (aka, between subjects). Boxplots should be evenly scattered around 0 for each subject.

```{r}
plot(me1, id ~ resid(.))
```

We may also want to assess that the variance (noise) comes from a Normal distribution. We need to use a function in the lattice package called `qqmath()`. (The lattice package comes with R.)

Remember we have two sources of variation (or random effects): Residuals (within subjects) and id (between subjects). To check Residual normality, we use the `qqmath()` function in the lattice package. The points should lie close to the diagonal line.

```{r}
lattice::qqmath(me1)
```

To assess the normality assumption of the subjects' random effects we again use the `qqmath()` function, but first have to extract the estimated random effects from the model object using the `ranef()` function. 

Remember the subject random effects we generated: `subj_noise`?

```{r}
subj_noise
```

The model also tries to estimate those values as well. We can see them using the `ranef()` function.

```{r}
ranef(me1)
```

This is similar to the previous plot but only has 7 points (one for each subject). It also has a +/- 1 standard deviation bar to give some sense of the uncertainty in the estimate of each subject's random effect. We'd like those point to roughly form a diagonal line.

```{r}
lattice::qqmath(ranef(me1))
```


## Using our model for predictions

In our basic linear model, we can make predictions with the `predict()` function. Below we predict y given x = 3 and request a 95% confidence interval.

```{r}
predict(m, newdata = data.frame(x = 3), 
        interval = "confidence")
```

_Predictions with mixed-effect/multilevel models require extra thought_. We need to decide whether or not we want to condition on random effects. In other words, are we making a prediction....

- for any subject, or 
- for a particular subject, such as subject 1 (id = 1)?

To make a prediction for any subject, perhaps a new subject who wasn't sampled, we specify `re.form=NA`. This says just use fixed effects to make a prediction. Predicted expected y when x = 3 for any subject:

```{r}
predict(me1, newdata = data.frame(x = 3),
        re.form=NA)
```

To make a prediction for each subject _using their random intercept_, we drop the `re.form=NA` argument and include `id = 1:7` in our new data frame. Predict expected y when x = 3 for each subject.

```{r}
predict(me1, newdata = data.frame(x = 3, id = 1:7))
```

From the lme4 documentation for predict: "There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters." That means we cannot simply ask for confidence intervals for our predictions. We can however use the bootstrap.

We can also visualize this model with ggeffects. By default it assumes predictions do not make use of random effects.

```{r}
ggpredict(me1, terms = "x") |>
  plot(add.data = TRUE)
```

To incorporate the additional uncertainty, we set `type = "random"` (for "random effects"). Notice the confidence ribbon is much wider.

```{r}
ggpredict(me1, terms = "x", type = "random") |>
  plot(add.data = TRUE)
```


It takes more work to get fitted lines for each subject. We need to specify `type = "random"` and add "id" to the `terms` argument. Notice each fitted line has a different intercept but the same slope. 

```{r}
ggpredict(me1, terms = c("x", "id"), type = "random") |> 
  plot(ci = FALSE, add.data = TRUE)
```


## CODE ALONG 1

Run the following code to load a new data set.

```{r}
d3 <- read.csv("data/d.csv", 
               colClasses = c("character", "numeric", "numeric"))
str(d3)
```

Add new R code chunks by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Visualize the data using ggplot2. Group by id.


2. Model y as a function of x with a random intercept conditional on id. Call the model `me3`.


3. Check the constant variance assumption for the observations. What do you think?


4. Now fit a model that assumes the subject exerts a random effect on both the intercept and slope, using the formula `y ~ x + (x|id)`. Call the model `me4`.


5. Check the assumption of constant variance for the observation random effect again. What do you think?


6. Plot the fitted model _for each subject_ using ggeffects.

## Mixed-effect/multilevel modeling with real data

The following data consist of 5 weekly measurements of body weight for 27 rats. The first 10 rats are on a control treatment while 7 rats have thyroxine added to their drinking water. 10 rats have thiouracil added to their water. We're interested in how the treatments affect the weight of the rats. Source: faraway package (Faraway, 2006)

```{r}
ratdrink <- read.csv("data/ratdrink.csv")
ratdrink$subject <- as.character(ratdrink$subject)
str(ratdrink)
```

Our data has 135 observations, but these are _not independent_. We have 27 subjects, and 5 observations on each of these subjects.

Let's visualize the data grouped by rat. It appears the trajectories of growth change depending on treatment.

```{r}
ggplot(ratdrink) +
  aes(x=weeks, y=wt, color=treat, group=subject) +
  geom_point() + 
  geom_line()
```

We may also want to visualize trend lines grouped by treat.

```{r}
ggplot(ratdrink) +
  aes(x = weeks, y = wt, color = treat) +
  geom_point() +
  geom_smooth(se = FALSE)
```


Again, we're interested in how the treatments affect the weight of the rats.

Let's model wt as a function of treat and weeks and let the intercept be conditional on the random effects of subject. This model allows each rat to have their own intercept but the same effects for treat and weeks.

```{r}
lmm1 <- lmer(wt ~ treat + weeks + (1 | subject), data=ratdrink)
summary(lmm1, corr=FALSE) # suppress "Correlation of Fixed Effects"
```

Is this model any good? One way to check is to simply check the constant variance assumption of the Residuals. This doesn't look good.

```{r}
plot(lmm1)
```

Another approach is to plot observed versus fitted values by subject. This is sometimes called a "clibration plot." A good model should return values with close to a 1:1 correspondence. It looks like some are ok, but others curve around the line.

```{r}
plot(lmm1, wt ~ fitted(.) | subject, abline = c(0,1))
```

Let's plot the fitted model using ggeffects.

```{r}
ggpredict(lmm1, terms = c("weeks", "treat")) |> 
  plot(ci = FALSE, add.data = TRUE)
```

## CODE ALONG 2

It appears the effect of weeks may depend on treat. 

1. Let's fit a model with an interaction between treat and weeks, and let's leave the intercept conditional on subject. Call the model `lmm2`

```{r}
lmm2 <- lmer(wt ~ treat * weeks + (1 | subject), data=ratdrink)
summary(lmm2, corr = FALSE)
```

2. Is this model "good"? Check the Residual plot.

```{r}
plot(lmm2)
```

3. Make a calibration plot.

```{r}
plot(lmm2, wt ~ fitted(.) | subject, abline = c(0,1))
```


4. Create an effect plot using ggeffects.

```{r}
ggpredict(lmm2, terms = c("weeks", "treat")) |>
  plot(ci = FALSE, add.data = TRUE)
```

5. what's the expected mean weight of a rat on thiouracil at week 4?

```{r}
predict(lmm2, newdata = data.frame(weeks = 4, treat = "thiouracil"),
        re.form = NA)
```

## Interpreting model coefficients

Recall the model summary:

```{r}
summary(lmm2, corr = FALSE)
```

Since we have interactions, it takes some work to interpret the coefficients.

- The `Intercept` is the expected weight of a rat at week 0 is the control group: about 52.8 grams
- The `weeks` coefficient is the expected amount of weight in grams a rat in the control group adds each week: about 26.5 grams.

- The `treatthiouracil` coefficient is what we _add_ to the `Intercept` to get the expected weight of a rat at week 0 is the thiouracil group: about 52.8 + 4.8 = 57.6 grams
- The `treatthiouracil:weeks` coefficient is what we _add_ to the `weeks` coefficient to the expected amount of weight in grams a rat in the thiouracil group adds each week: about 26.5 + -9.4 = 17.1 grams

The same calculations can be done for the thyroxine coefficients.

The "Std. Error" column quantifies the uncertainty in the coefficient. The "t value" is the ratio Estimate/Std. Error. A t value greater than about 3 in absolute value provides evidence the estimated coefficient is different from 0. In other words, it's more than three standard errors away from 0.

_Where are the p-values?_ In mixed-effect models the distribution of t values for the null hypothesis is not known, at least not for unbalanced data. P-values can be approximated, but not calculated precisely. The `lme4` authors elected to not output p-values. Probably better to look at confidence intervals anyway.

We can get confidence intervals for the coefficients by using the `confint` function. Notice we get confidence intervals for the random effect estimates as well. Setting `oldNames = FALSE` returns more informative names for the random effects.

```{r}
confint(lmm2, oldNames = TRUE)
```

