---
title: "Mixed-Effect/Multilevel Modeling with R"
author: "Clay Ford, UVA Library"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## Load packages

We're going to use the following packages in this workshop. Let's load them.

```{r}
library(lme4)
library(ggplot2)
library(ggeffects)
library(emmeans)
```



## Simple linear regression review

Instead of using mathematical statistics, we'll try to motivate mixed-effect/multilevel models using simulation. Below we generate data from a straight line model. y is completely determined by x. We might say the intercept (3) and slope (2) are _fixed effects_.

```{r}
x <- 1:10
y <- 3 + 2*x
d <- data.frame(x, y)
plot(y ~ x, data = d)
```


Now let's add some noise to each observation. We'll use 10 random draws from a Normal distribution with mean 0 and standard deviation of 1.5. The `set.seed(1)` function ensures we all draw the same values. Now y looks associated with x, but not completely determined by x. We might say each observation has a _random effect_.

```{r}
set.seed(1)
noise <- rnorm(10, mean = 0, sd = 1.5)
d$y <- 3 + 2*x + noise
plot(y ~ x, data = d)
```

Now let's fit a simple linear model, or regression line, using the correct model we used to simulate the data. We use the `lm()` function for this. The formula "y ~ x" means we think the model is "y = intercept + slope*x".

```{r}
m <- lm(y ~ x, data = d)
summary(m)
```

This is one way to think of linear modeling or regression: we try to recover the process that generated the data.

Assuming our data came from a function of the form _y = a + b*x_ with noise sampled from a Normal distribution, the model estimates a as 2.7468 and b as 2.0821. Those are fairly close to the true values of 3 and 2. The noise is estimated to be from a Normal distribution with mean 0 and standard deviation 1.214 (Residual standard error). Again close to the true value of 1.5.

We might say the _fixed effects_ are about 2.75 and 2.08, and the _random effect_ is 1.24.

If we like we can fit this model to the data using the ggeffects package.

```{r}
plot(ggpredict(m, terms = "x"), add.data = TRUE)

```


## Motivation for mixed-effect/multilevel models

Pretend the previous example data was for one subject. What if we had 7 subjects, each with 10 observations? 

Let's simulate data where we add noise to the intercept that's _specific to each subject_. In other words each subject has a _random effect_. 


```{r}
# generate 7 id numbers, 10 each
id <- gl(n = 7, k = 10)

# repeat x times
x <- rep(1:10, 7)

# generate a random effect specific to each observation (n = 10 * 7)
set.seed(2)
obs_noise <- rnorm(7 * 10, mean = 0, sd = 1.5)

# generate a random effect specific to each subject (n = 7)
set.seed(3)
subj_noise <- rnorm(7, mean = 0, sd = 2)

# generate y; add subject random effect to intercept;
# rand_int[id] uses id as index numbers to repeat subj_noise values
y <- (3 + subj_noise[id]) + 2*x + obs_noise
d2 <- data.frame(id, y, x)
```

Now we have two fixed effects:
- Intercept = 3
- Slope = 2

And two random effects:
- Observation: N(0, 1.5)
- Subject: N(0, 2)

Let's visualize the data. We have lines with _different intercepts_ but similar slopes. That's because _we added each subject's random effect to the fixed intercept_. Any differences in slopes are due to random effects associated with each observation.

The argument `method = "lm"` below adds a regression line to each subject's values.

```{r}
ggplot(d2) +
  aes(x = x, y = y, color = id) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

How do we "work backwards" as we did the simple linear model and recover the true intercept, slope and two random effect parameters?

**linear Mixed-Effect Models, or Multilevel Models**

Because we are dealing with models with a mix of fixed and random effects, we sometimes call these **Mixed-Effect Models**. Another name is **Multilevel Models** because we're dealing with different levels of observations. (eg, we observe 7 subjects, but also observe 10 observations on each subject.)

Below we model y as a function of x plus an intercept (1) that is conditional on id (the subject) using the `lmer()` function from the lme4 package. Notice this is the _correct model_! The y value really is a function of x plus an intercept that varies between subjects.  

```{r}
# (1|id) = intercept conditional on id
me1 <- lmer(y ~ x + (1|id), data = d2)
summary(me1)
```

The _Fixed Effects_ section says that assuming this data came from a straight line model with an intercept and slope, the model estimates the intercept to be 2.65 and the slope to be 1.97. These are very close to the true values of 3 and 2. 

The _Random Effects_ section reports two standard deviations: one for id and and one called "Residual". The first is the estimate of the standard deviation of the normal distribution from which the subject-specific noise was drawn: 1.157. The second is the estimate of the standard deviation of the normal distribution from which the residual noise was drawn: 1.795. Recall the true values were 2 and 1.5, respectively. 

If we look at the coefficients of this model, notice everyone got their _own intercept_ but has the same slope. We have essentially fit a straight line model to each subject. This is sometimes called a _random intercept model_. Each subject has a random effect associated with the intercept.

```{r}
coef(me1)
```


This is one way to think of mixed-effect/multilevel modeling: _trying to work backward to determine the data generation process_. Our example was simple and we knew the data generating process because we simulated the data. In real life we never know if our model is correct (spoiler: it never is) and the data generating process is complicated.

## Checking assumptions

Notice two assumptions that `lmer()` made:

1. The variance (noise) is constant for both random effects.
2. The variance (noise) comes from a Normal distribution for both random effects

We can assess those assumptions with _diagnostic plots_.

Check constant variance for Residuals (aka, within subjects). Points should be evenly scattered around 0.

```{r}
plot(me1)
```

Check constant variance for Subjects (aka, between subjects). Boxplots should be evenly scattered around 0 for each subject.

```{r}
plot(me1, id ~ resid(.))
```

To assess that the variance (noise) comes from a Normal distribution, we need to use a function in the lattice package called `qqmath()`. (The lattice package comes with R.)

Remember we have two sources of variation (or random effects): Residuals (within subjects) and id (between subjects). To check Residual normality, we use the `qqmath()` function in the lattice package. The points should lie close to the diagonal line. The syntax `lattice::` allows us to access the function from the package without loading the package.

```{r}
lattice::qqmath(me1)
```

To assess the normality assumption of the subjects' random effects we again use the `qqmath()` function, but first have to extract the estimated random effects from the model object using the `ranef()` function. 

Remember the subject random effects we generated: `subj_noise`?

```{r}
subj_noise
```

The model also tries to estimate those values as well. We can see them using the `ranef()` function.

```{r}
ranef(me1)
```

Feed the output of that function to `qqmath`.

```{r}
lattice::qqmath(ranef(me1))
```

This is similar to the previous plot but only has 7 points (one for each subject). It also has a +/- 1 standard deviation bar to give some sense of the uncertainty in the estimate of each subject's random effect. We'd like those points to roughly form a diagonal line.

## Using our model for predictions

In our basic linear model, we can make predictions with the `predict()` function. Below we predict y given x = 3 and request a 95% confidence interval. In other words, what's the expected mean of y given x = 3?

```{r}
predict(m, newdata = data.frame(x = 3), 
        interval = "confidence")
```

_Predictions with mixed-effect/multilevel models require extra thought_. We need to decide whether or not we want to condition on random effects. In other words, are we making a prediction....

- for any subject, or 
- for a particular subject, such as subject 1 (id = 1)?

To make a prediction for any subject, perhaps a new subject who wasn't sampled, we specify `re.form=NA`. This says just use fixed effects to make a prediction. Predict expected y when x = 3 for any subject:

```{r}
predict(me1, newdata = data.frame(x = 3),
        re.form=NA)
```

To make a prediction for each subject _using their random intercept_, we drop the `re.form=NA` argument and include `id = 1:7` in our new data frame. Predict expected y when x = 3 for each subject.

```{r}
predict(me1, newdata = data.frame(x = 3, id = 1:7))
```

From the lme4 documentation for predict: "There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters." That means we cannot simply ask for confidence intervals for our predictions. We can however use the bootstrap. (see below)

We can also visualize this model with ggeffects. By default it assumes predictions do not make use of random effects. Below we use `ggpredict` to make predictions for various values of x, and pipe the result into `plot`. The `add.data` argument adds the raw data to the plot.

```{r}
ggpredict(me1, terms = "x") |>
  plot(add.data = TRUE)
```

To incorporate the additional uncertainty due to each subject's random effect, we set `type = "random"` (for "random effects"). Notice the confidence ribbon is much wider.

```{r}
ggpredict(me1, terms = "x", type = "random") |>
  plot(add.data = TRUE)
```


It takes more work to get fitted lines for each subject. We need to specify `type = "random"` and add "id" to the `terms` argument. Notice each fitted line has a different intercept but the same slope. (Honestly, this isn't something that's done very often in practice.)

```{r}
ggpredict(me1, terms = c("x", "id"), type = "random") |> 
  plot(ci = FALSE, add.data = TRUE)
```


## CODE ALONG 1

Run the following code to load a new data set. 

```{r}
d3 <- read.csv("data/d.csv")
d3$id <- as.character(d3$id)
str(d3)
```

Add new R code chunks by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac). 

1. Visualize the data using ggplot2. Group by id.


2. Model y as a function of x with a random intercept conditional on id. Call the model `me3`.


3. Check the constant variance assumption for the observations. What do you think?


4. Now fit a model that assumes the subject exerts a random effect on both the intercept and slope, using the formula `y ~ x + (x|id)`. Call the model `me4`.


5. Check the assumption of constant variance for the observation random effect again. What do you think?


6. Plot the fitted model _for each subject_ using ggeffects.



## Mixed-effect/multilevel modeling with real data

Let's look at some realistic data.

The following data consist of 5 weekly measurements of body weight for 27 rats. 10 rats are on a control treatment. 7 rats have thyroxine added to their drinking water. 10 rats have thiouracil added to their water. We're interested in how the treatments affect the weight of the rats. Source: faraway package (Faraway, 2006)

```{r}
ratdrink <- read.csv("data/ratdrink.csv")
ratdrink$subject <- as.character(ratdrink$subject)
str(ratdrink)
```

Our data has 135 observations, but these are _not independent_. We have 27 subjects, and 5 observations on each of these subjects.

Let's visualize the data grouped by rat. It appears the trajectories of growth change depending on treatment.

```{r}
ggplot(ratdrink) +
  aes(x=weeks, y=wt, color=treat, group=subject) +
  geom_point() + 
  geom_line()
```

We may also want to visualize trend lines grouped by treat.

```{r}
ggplot(ratdrink) +
  aes(x = weeks, y = wt, color = treat) +
  geom_point() +
  geom_smooth(se = FALSE)
```


Again, we're interested in how the treatments affect the weight of the rats.

Let's model wt as a function of treat and weeks and let the intercept be conditional on the random effects of subject. This model allows each rat to have their own intercept but the same effects for treat and weeks. This model says the effect of weeks is the same regardless of treatment.

```{r}
lmm1 <- lmer(wt ~ treat + weeks + (1 | subject), data=ratdrink)
summary(lmm1, corr=FALSE) # suppress "Correlation of Fixed Effects"
```

Naive interpretation:

- Rats gain about 23 grams per week
- mean weight at week 0 for control group is about 59 (Intercept)
- mean weight at week 0 for thiouracil group is about 59 - 13
- mean weight at week 0 for thyroxine group is about 59 + 0.5

Is this model any good? One way to check is to simply check the constant variance assumption of the Residuals. This doesn't look good.

```{r}
plot(lmm1)
```

Let's plot the fitted model using ggeffects. 

```{r}
ggpredict(lmm1, terms = c("weeks", "treat")) |> 
  plot(ci = FALSE, add.data = TRUE)
```

## CODE ALONG 2

It appears the effect of weeks may depend on treat. 

1. Let's fit a model with an interaction between treat and weeks, and let's leave the intercept conditional on subject. Call the model `lmm2`

```{r}
lmm2 <- lmer(wt ~ treat * weeks + (1 | subject), data=ratdrink)
summary(lmm2, corr = FALSE)
```

2. Is this model "good"? Check the Residual plot.

```{r}
plot(lmm2)
```

3. Create an effect plot using ggeffects.

```{r}
ggpredict(lmm2, terms = c("weeks", "treat")) |>
  plot(ci = FALSE, add.data = TRUE)
```

4. what's the expected mean weight of a rat at week 4 for each treatment?

```{r}
g <- unique(ratdrink$treat)
predict(lmm2, newdata = data.frame(weeks = 4, treat = g),
        re.form = NA)
```

## Interpreting model coefficients

Recall the model summary:

```{r}
summary(lmm2, corr = FALSE)
```

Since we have interactions, it takes some work to interpret the coefficients.

- The `Intercept` is the expected weight of a rat at week 0 is the control group: about 52.8 grams
- The `weeks` coefficient is the expected amount of weight in grams a rat in the control group adds each week: about 26.5 grams.

- The `treatthiouracil` coefficient is what we _add_ to the `Intercept` to get the expected weight of a rat at week 0 is the thiouracil group: about 52.8 + 4.8 = 57.6 grams
- The `treatthiouracil:weeks` coefficient is what we _add_ to the `weeks` coefficient to the expected amount of weight in grams a rat in the thiouracil group adds each week: about 26.5 + -9.4 = 17.1 grams

The same calculations can be done for the thyroxine coefficients.

The "Std. Error" column quantifies the uncertainty in the coefficient. The "t value" is the ratio Estimate/Std. Error. A t value greater than about 3 in absolute value provides evidence the estimated coefficient is different from 0. In other words, it's more than three standard errors away from 0.

_Where are the p-values?_ In mixed-effect models the distribution of t values for the null hypothesis is not known, at least not for unbalanced data. P-values can be approximated, but not calculated precisely. The `lme4` authors elected to not output p-values. Probably better to look at confidence intervals anyway.

We can get confidence intervals for the coefficients by using the `confint` function. Notice we get confidence intervals for the random effect estimates as well. Setting `oldNames = FALSE` returns more informative names for the random effects.

```{r}
confint(lmm2, oldNames = FALSE)
```

## Comparing models

We have many options when building a mixed-effect model. In addition to what predictors to include and whether they should interact, we get to decide how many random effects to allow. The previous model allowed for one random effect on the intercept. 

Let's allow a random effect on weeks as well. This says the effect of weeks is conditional on each rat. (This may not be correct.)

```{r}
lmm3 <- lmer(wt ~ treat * weeks + (weeks | subject), data=ratdrink)
summary(lmm3, corr = FALSE)
```

Under Random Effects we have three estimates of "noise" or variance. We can think of those values as the standard deviations of Normal distributions (with mean 0) from which random values were drawn. Notice also the random effects for the intercept and weeks are assumed to be correlated. 

If we look at the coefficients, each rat now has their own intercept and weeks coefficients. 

```{r}
coef_out <- coef(lmm3)
coef_out$subject
```

Is this model better than the model with just a random intercept? One way to assess this is with selection criteria such as AIC or BIC. These are basically measures of well the model would fit new data. Lower values mean a better "fit". An AIC/BIC value by itself doesn't mean much. But if we have multiple AIC/BIC values from different models fit to the same data with the same response, we can use them to help us select a "preferred" model.

AIC stands for Akaike Information Criterion. BIC stands for Bayesian Information Criterion. Each can be obtained using `AIC()` or `BIC()`, respectively. The df column in the output refers to degrees of freedom. That's the number of parameters our model estimated. More parameters means a more complicated model. 

```{r}
AIC(lmm2, lmm3)
```

```{r}
BIC(lmm2, lmm3)
```

In both cases it looks like `lmm3` is the better model. It seems the random effect for weeks is justified. 

In this updated model the intercept and weeks random effects are _assumed to be correlated_. We can extract just the random effect variance estimates with the `VarCorr()` function. This is the portion presented in the Random Effects section of the summary output. 

```{r}
VarCorr(lmm3)
```

The correlation estimate is small, only -0.133. Correlated random effects in this case means that subjects' random effects on the intercept and weeks coefficients are associated. For example, if one subject has a large random effect on the intercept, they may have a large random effect on the weeks coefficient. 

If we like we can fit a model with no correlation between random effects. To do this we use two pipes in the random effects formula: `(weeks || subject)`. (This only works for numeric predictors.)

```{r}
lmm4 <- lmer(wt ~ treat * weeks + (weeks || subject), data=ratdrink)
VarCorr(lmm4)
```

How does this model compare to the model with correlated random effects?

```{r}
AIC(lmm3, lmm4)
```

It seems slightly better, though that may be due to the sample. Probably doesn't make a difference. 

## CODE ALONG 3

1. Fit a model with the random effect on weeks but not on the intercept: `(0 + weeks | subject)`. Call the model lmm5.

```{r}
lmm5 <- lmer(wt ~ treat * weeks + (0 + weeks | subject), data = ratdrink)
summary(lmm5)
```

2. How does this model compare to `lmm3`, which has a random effect on both intercept and weeks?

```{r}
AIC(lmm5, lmm3)
```

## Making comparisons

Let's say we're happy with model `lmm3`. It's good enough. How can we use that model to make comparisons between treatments at certain times? How can we compare trends over time between treatments?  

A package that is useful for this is the **emmeans** package. That's short for Estimated Marginal Means. emmeans is a large and powerful package. We show two basic uses in this example.

It appears at week 4, the end of the observation period, rats on thiouracil weigh less than rats on the other two treatments. How much less? We use the `emmeans()` function to analyze this. It takes a fitted model as the first argument. The second argument uses emmeans syntax to specify we want `pairwise` comparisons between all three treatments. The `at` argument allows us to specify at what time point we want to make the comparisons. Notice this needs to be a data frame. The note about results being misleading can be ignored. 

```{r}
emmeans(lmm3, pairwise ~ treat, at = (data.frame(weeks = 4)))
```

The first section returns the estimated mean weight for each treatment at week 4. The second section compares those weights. The mean weight for rats on thiouracil is 126. That's about 32 grams less than the control and thyroxine groups. Those differences appear to be significant judging by p-values. NOTE: The p-values are approximate. Recall we can't precisely calculate p-values for mixed-effect models with unbalanced groups. The Kenward-Roger method is one approach for obtaining approximate p-values.

Probably better to look at confidence intervals. Below we pipe the previous result into `confint()`.

```{r}
emmeans(lmm3, pairwise ~ treat, at = (list(weeks = 4))) |>
  confint()
```

It appears that rats on thiouracil can be expected to weigh at least 15 grams less than the other two groups at 4 weeks.

To compare the trends (or slopes) over time, we use the `emtrends()` function. The syntax is almost the same as before, except this time we use the `var` argument to specify the x-axis of the trend line. In this case it's weeks.

```{r}
emtrends(lmm3, pairwise ~ treat, var = "weeks")
```

The trend or slope of thiouracil appears to be about 10 units less than the other two groups: 17 versus 27. That difference appears to be significant according to p-values. Again we can pipe the result into `confint()` to get confidence intervals.

```{r}
emtrends(lmm3, pairwise ~ treat, var = "weeks") |>
  confint()
```

The difference in trend looks to be at least 5 between thioruacil and the other two groups.

## Using Simulation with Mixed-Effect Models

If our mixed-effect model is good, it should generate data similar to our observed data. The `simulate()` function allows us to use our model to rapidly generate new responses using our model. We can then compare the distribution of the model generated responses to the distribution of our observed response data.

Let's try `lmm3`.

```{r}
sim1 <- simulate(lmm3, nsim = 50)
plot(density(ratdrink$wt))
for(i in 1:50)lines(density(sim1[[i]]), col = "grey80")
```

Let's try a different model. Below we fit a model that doesn't take treat into account.

```{r}
lmmX <- lmer(wt ~ weeks + (1|subject), data = ratdrink)
```

Model comparison via AIC tells us `lmm3` is better. 

```{r}
AIC(lmm3, lmmX)
```
But in what way? Is `lmmX` necessarily bad? Let's simulate wt using the `lmmX` model and overlay the model generated distributions on top of the observed distribution. 

```{r}
sim2 <- simulate(lmmX, nsim = 50)
plot(density(ratdrink$wt))
for(i in 1:50)lines(density(sim2[[i]]), col = "grey80")
```

It doesn't look horrible, but in the 100 - 200 range the observed data does not seem consistent with the model generated data. The `lmm3` model with treat does a better job of capturing the variation in rats as they get heavier.

Another use for simulation is to approximate confidence intervals for predictions. Recall the usual `predict` method for `lmer()` models does not compute standard errors, and hence does not calculate confidence intervals.

To do this we'll use the `bootMer()` function. This is like `simulate()` but goes a step further. After each simulation of new responses, a new model is refit. Once we fit a new model, we can make a prediction. We can then repeat this process many times to get many simulated predictions. We can then use all those predictions to make inference about the uncertainty of our predictions. This is referred to as a _model-based bootstrapping_. And it's probably easier to understand with a demonstration.

Let's say we want to calculate a confidence interval on the difference in means between thiouracil and thyroxine at week 4 using our `lmm3` model. Here's one way using `bootMer()`.

First we define two data frames, `nd1` and `nd2`, with the predictors of interest. Next we tell the `bootMer()` function to use the `lmm3` model to simulate data and then refit the model. The `FUN` argument tells `bootMer()` what to do with the newly fit model. In this case, make a prediction using our two datasets and take the difference. The `nsim` argument says do it 100 times. (We would typically do something like a 1000, but in the interest of time we do 100.). The `.progress` argument says output a text-based progress bar so we can keep tabs on progress.

```{r}
nd1 <- data.frame(treat="thiouracil", weeks=4)
nd2 <- data.frame(treat="thyroxine", weeks=4)
b.out <- bootMer(x = lmm3, 
                 FUN = function(x){
                   predict(x, newdata = nd1, re.form=NA) -
                     predict(x, newdata = nd2, re.form=NA)}, 
                 nsim = 100, 
                 .progress = "txt")

```

When this finishes running the `b.out` object is a list with several objects. The `t` object is the vector of differences. Using the quantile function we can extract the 2.5% and 97.5% percentiles to construct a confidence interval.

```{r}
head(b.out$t)
quantile(b.out$t, probs = c(0.025, 0.975))
```

The confidence interval returned by emmeans was [-53.9, -15.2]. Recall the emmeans method used the Kenward-Roger approximation. The `bootMer()` confidence interval is a good deal smaller. 
