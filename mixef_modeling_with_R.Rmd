---
title: "Mixed-Effect/Multilevel Modeling with R "
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter* (Win/Linux) or *Cmd+Shift+Return* (Mac). 

```{r}
plot(cars)
```

To hide the output, click the Expand/Collapse output button. To clear results (or an error), click the "x". 

You can also press *Ctrl+Enter* (Win/Linux) or *Cmd+Return* (Mac) to run one line of code at a time (instead of the entire chunk).

Add a new R code chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I* (Win/Linux) or *Cmd+Option+I* (Mac).  

## Load packages

We're going to use the following packages in this workshop. Let's load them.

```{r}
library(lme4)
library(ggplot2)
library(ggeffects)
```



## Simple linear regression review

Instead of using mathematical statistics, we'll try to motivate mixed-effect/multilevel models using simulation. Below we generate data from a straight line model. y is completely determined by x.

```{r}
x <- 1:10
y <- 3 + 2*x
d <- data.frame(x, y)
plot(y ~ x, data = d)
```


Now let's add some noise. We'll use 10 random draws from a Normal distribution with mean 0 and standard deviation of 1.5. The `set.seed(1)` function ensures we all draw the same values. Now y looks associated with x, but not completely determined by x.

```{r}
set.seed(1)
noise <- rnorm(10, mean = 0, sd = 1.5)
d$y <- 3 + 2*x + noise
plot(y ~ x, data = d)
```

Now let's fit a simple linear model, or regression line, using the correct model we used to simulate the data. We use the `lm()` function for this. The formula "y ~ x" means we think the model is "y = intercept + slope*x".

```{r}
m <- lm(y ~ x, data = d)
summary(m)
```

Assuming our data came from a function of the form y = a + b*x with noise sampled from a Normal distribution, the model estimates a as 2.7468 and b as 2.0821. Those are fairly close to the true values of 3 and 2. The noise is estimated to be from a Normal distribution with mean 0 and standard deviation 1.214. Again close to the true value of 1.5.

If we like we can fit this model to the data using the ggeffects package.

```{r}
plot(ggeffect(m, terms = "x"), add.data = TRUE)

```


## Motivation for mixed-effect/multilevel models

Pretend the previous example data was for one subject. What if we had 7 subjects? How might we simulate that data? Maybe we want to allow subjects to have a different intercept, or a different slope, or both? 

Let's simulate data where we add noise to the intercept that's _specific to each subject_.

```{r}
# generate 7 id numbers, 10 each
id <- gl(n = 7, k = 10)

# repeat x times
x <- rep(1:10, 7)

# generate noise: 10 * 7 = 70 total observation
set.seed(2)
noise <- rnorm(7 * 10, mean = 0, sd = 1.5)

# generate subject-specific noise; only 7 values
set.seed(3)
rand_int <- rnorm(7, mean = 0, sd = 2)

# generate y; add subject-specific noise to intercept
y <- 3 + rand_int[id] + 2*x + noise
d2 <- data.frame(id, y, x)
d2
```

Let's visualize the data. We have lines with _different intercepts_ but similar slopes. The argument `method = "lm"` below adds a regression line to each subject's values.

```{r}
ggplot(d2) +
  aes(x = x, y = y, color = id) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

How do we "work backwards" and recover the true intercept, slope and two noise parameters?

**linear mixed-effect models, or multilevel models**

Below we model y as a function of x plus an intercept (1) that is conditional on id (the subject) using the `lmer()` function from the lme4 package. Notice this is the _correct model_! The y value really is a function of x plus an intercept that varies between subjects.  

```{r}
me1 <- lmer(y ~ x + (1|id), data = d2)
summary(me1)
```

The summary says that assuming this data came from a straight line model with an intercept and slope, the model estimates the intercept to be 2.65 and the slope to be 1.97 (under _Fixed Effects_). These are very close to the true values of 3 and 2. 

The summary also reports two standard deviations: one for id and and one called "Residual" (under _Random Effects_). The first is the estimate of the standard deviation of the normal distribution from which the subject-specific noise was drawn: 1.157. The second is the estimate of the standard deviation of the normal distribution from which the residual noise was drawn: 1.795. Recall the true values were 2 and 1.5, respectively. 

If we look at the coefficients of this model, notice everyone got their _own intercept_ but has the same slope. We have essentially fit a straight line model to each subject. This is sometimes called a _random intercept model_. Each subject has a random effect associated with the intercept.

```{r}
coef(me1)
```


This is basically mixed-effect/multilevel modeling: _trying to work backward to determine the data generating process_. Our example was simple and we knew the data generating process because we simulated the data. In real life we never know if our model is correct (spoiler: it never is) and the data generating process is complicated.

## Checking assumptions

Notice the assumptions that `lmer()` made:

1. The variance (noise) is constant, both within and between subjects
2. The variance (noise) comes from a Normal distribution, both within and between subjects

We can check those assumptions with diagnostic plots.

Check constant variance within subjects. Points should be evenly scattered around 0.

```{r}
plot(me1)
```

Check constant variance _between subjects_. Boxplots should be evenly scattered around 0 for each subject.

```{r}
plot(me1, id ~ resid(.))
```

Check that the variance (noise) comes from a Normal distribution. We need to use a function in the lattice package called `qqmath()`

Remember we have two sources of variation: within subjects (Residuals) and between subjects (id). To check within subject normality, we use the `qqmath()` function in the lattice package. The points should lie close to the diagonal line.

```{r}
lattice::qqmath(me1)
```

To assess the normality assumption of the between subject variance we again use the `qqmath()` function but first have to extract the random effects from the model object using the `ranef()` function. This is similar to the previous plot but only has 7 points (one for each subject). It also has a +/- 1 standard deviation bar to give some sense of the uncertainty in the estimate. We'd like those point to roughly form a diagonal line.

```{r}
lattice::qqmath(ranef(me1))

```

## Using our model for predictions

In our basic linear model, we can make predictions with the `predict()` function. Below we predict y given x = 3 and request a 95% confidence interval.

```{r}
predict(m, newdata = data.frame(x = 3), 
        interval = "confidence")
```

Predictions with mixed-effect/multilevel models require a little extra thought. We need to decide whether or not we want to condition on random effects. In other words, are we making a prediction for any subject, or for a particular subject, such as subject 1 (id = 1)?

To make a prediction for any subject, perhaps a new subject who wasn't sampled, we specify `re.form=NA`. This says just use fixed effects to make a prediction.

```{r}
predict(me1, newdata = data.frame(x = 3),
        re.form=NA)
```

To make a prediction for each subject _using their random intercept_, we drop the `re.form=NA` argument and include `id = 1:7` in our new data frame.

```{r}
predict(me1, newdata = data.frame(x = 3, id = 1:7))
```

From the lme4 documentation for predict: "There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters." That means we cannot simply ask for confidence intervals for our predictions. We can however use the bootstrap.

We can also visualize this model with ggeffects. By default it assumes predictions do not make use of random effects.

```{r}
ggpredict(me1, terms = "x") |>
  plot(add.data = TRUE)
```

To incorporate the additional uncertainty, we set `type = "re"` (for "random effects"). Notice the confidence ribbon is much wider.

```{r}
ggpredict(me1, terms = "x", type = "re") |>
  plot(add.data = TRUE)
```


It takes a little more work to get fitted lines for each subject. We need to specify `type = "re"` and add "id" to the `terms` argument. Notice each fitted line has a different intercept but the same slope. 

```{r}
ggpredict(me1, terms = c("x", "id"), type = "re") |> 
  plot(ci = FALSE, add.data = TRUE)
```


## CODE ALONG 1

Let's simulate data where subjects have random slopes and random intercepts.

```{r}
id <- gl(n = 7, k = 10)
x <- rep(1:10, 7)
set.seed(2)
noise <- rnorm(7 * 10, mean = 0, sd = 1.5)
set.seed(3)
rand_int <- rnorm(7, mean = 0, sd = 2)

# random effect for slope
set.seed(4)
rand_slope <- rnorm(7, mean = 0, sd = 3)

# generate y; add subject-specific noise to intercept
y <- 3 + rand_int[id] + (2 + rand_slope[id])*x + noise
d3 <- data.frame(id, y, x)
d3
```

Now let's fit the correct model, with a random intercept and random slope. Let's name the model "me2"

```{r}
me2 <- lmer(y ~ x + (x||id), data = d3)
summary(me2)
```

Let's plot the fitted model _for each subject_ using ggeffects.

```{r}
ggpredict(me2, terms = c("x", "id"), type = "re") |>
  plot(ci = FALSE, add.data = TRUE)
```

## Mixed-effect/multilevel modeling with real data

